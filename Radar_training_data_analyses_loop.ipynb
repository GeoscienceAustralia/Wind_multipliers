{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar training data analyses.ipynb\n",
    "\n",
    "(Include description of code here!)\n",
    "\n",
    "By Sharon Jones, September 2017, Python v3, DEA v1.5.2, Radar data from `simoncube`\n",
    "\n",
    "Modified after `Check_for_statistical_difference_in_slope.ipynb` in Geoscience Australia's `GWBAGDC` Git repo. \n",
    "\n",
    "** Code dependencies **\n",
    "- training data shape files for different land cover types (derived from ArcGIS over the TC Debbie landfall region). The code will look for the training datasets in `/g/data1/w85/training_shapefiles/`. Ensure that al of the training shapefiles are executable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries we need in the code and tell matplotlib to display the plots here\n",
    "%matplotlib inline\n",
    "import fiona\n",
    "import shapely.geometry\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import geopandas as gp\n",
    "import datacube\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up some functions to use later in the code\n",
    "def warp_geometry(geom, src_crs, dst_crs):\n",
    "    \"\"\"\n",
    "    warp geometry from src_crs to dst_crs\n",
    "    \"\"\"\n",
    "    return shapely.geometry.shape(rasterio.warp.transform_geom(src_crs, dst_crs, shapely.geometry.mapping(geom)))\n",
    "\n",
    "def geometry_mask(geom, geobox, all_touched=False, invert=False):\n",
    "    \"\"\"\n",
    "    rasterize geometry into a binary mask where pixels that overlap geometry are False\n",
    "    \"\"\"\n",
    "    return rasterio.features.geometry_mask([geom],\n",
    "                                           out_shape=geobox.shape,\n",
    "                                           transform=geobox.affine,\n",
    "                                           all_touched=all_touched,\n",
    "                                           invert=invert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training data shapefiles and read in csv file with all area bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_types = [ 'water_areas', 'bare_areas', 'urban_areas', 'crop_areas', 'open_forest_areas','forest_areas']\n",
    "\n",
    "# Set up the case study bounding box (to make the file smaller and avoid memory errors)\n",
    "# Read in a csv file with bounding boxes\n",
    "\n",
    "names = pd.read_csv('/g/data/w85/radar_grad/bound_box.csv', delimiter = ',')\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through each bounding box, then each training data type\n",
    "Write out a pickle file with the results from each iteration of the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = len(names.index)\n",
    "iterable = list(range(0,x)) \n",
    "\n",
    "for num in iterable:\n",
    "    Studysite = names.ix[num]\n",
    "    print ('Working on ' + Studysite.Name)\n",
    "    for ttype in training_types:\n",
    "        print('Working on ' + ttype)\n",
    "        \n",
    "        ## We are going to read in the relevant shape file, and process it\n",
    "        shp = gp.GeoDataFrame.from_file('/g/data/w85/radar_grad/training_shapefiles_sj/' + ttype + '.shp')\n",
    "        # Create a bounding box from the locations specified above\n",
    "        box = shapely.geometry.box(names.min_lon[num], names.min_lat[num], names.max_lon[num], names.max_lat[num], ccw = True)\n",
    "        # Only get the polygons that intersect the bounding box (i.e. remove all the irrelevant ones)\n",
    "        filtered = shp.where(shp.intersects(box)).dropna()\n",
    "        # Combine all of the relevant polygons into a single polygon\n",
    "        shp_union = shapely.ops.unary_union(filtered.geometry)\n",
    "        \n",
    "        ## Check whether there are any training polygons in the box we are working on\n",
    "        if shp_union:\n",
    "            ## Now we will read data from DEA, or the pre-extracted pickel file\n",
    "            dc = datacube.Datacube(config='/g/data/u46/users/brl654/datacube/simoncube.conf')\n",
    "            cache = '/g/data/u46/users/sj9724/xarray.pickle.debbie.' + Studysite.Name\n",
    "            try:\n",
    "                with open(cache, 'rb') as file:\n",
    "                     z = pickle.load(file) # this only takes tens of seconds. (6GB)\n",
    "            except:\n",
    "                #this may be 5 to 10 mintues (seeking through half a terabyte)\n",
    "                z = dc.load(product='s1_gamma0_scene', lat = (names.max_lat[num], names.min_lat[num]), \n",
    "                            lon = (names.min_lon[num], names.max_lon[num]), output_crs='epsg:3577', \n",
    "                            resolution=(-100,100), resampling='average')\n",
    "                with open(cache, 'wb') as file:\n",
    "                    pickle.dump(z, file, protocol=-1) # save result to disk\n",
    "\n",
    "            ## This grabs the metadata for EPSG3577\n",
    "            from osgeo.osr import SpatialReference\n",
    "            # Get the WKT for EPSG:3577 (AGDC projection) \n",
    "            spatial_ref_object = SpatialReference()\n",
    "            spatial_ref_object.ImportFromEPSG(3577)\n",
    "            spatial_ref = spatial_ref_object.ExportToWkt()\n",
    "\n",
    "            # Create the mask based on our shapefile\n",
    "            mask = geometry_mask(warp_geometry(shp_union, shp.crs, spatial_ref), z.geobox, invert=True)\n",
    "            # Get data only where the mask is 'true'\n",
    "            data_masked = z.where(mask)\n",
    "            # Make a mean of all time\n",
    "            data_masked_mean = data_masked.mean('time')\n",
    "\n",
    "            ## Write out the results to a pickle file\n",
    "            outfile = '/g/data/u46/users/sj9724/radar_250m/xarray.pickle.debbie_{0}_{1}'.format(Studysite.Name, ttype) \n",
    "            print('writing to {0}'.format(outfile))\n",
    "            with open(outfile, 'wb') as file:\n",
    "                 pickle.dump(data_masked_mean, file, protocol=-1) # save result to disk\n",
    "        else:\n",
    "            print('There are no training polygons in this area for this type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data for each training type to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = len(names.index)\n",
    "iterable = list(range(0,x)) \n",
    "zs = dict()\n",
    "for ttype in training_types:\n",
    "    print('Working on ' + ttype)\n",
    "    zs[ttype] = list()\n",
    "    for num in iterable:\n",
    "        Studysite = names.ix[num]\n",
    "        infile = '/g/data/u46/users/sj9724/radar_250m/xarray.pickle.debbie_{0}_{1}'.format(Studysite.Name, ttype) \n",
    "        try:\n",
    "            with open(infile, 'rb') as file:\n",
    "                zs[ttype].append(pickle.load(file)) # this only takes tens of seconds. (6GB)\n",
    "                print('done!')\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zs2 = dict()\n",
    "for ttype in training_types:\n",
    "    data_final = []\n",
    "    print('Working on ' + ttype)\n",
    "    zs2[ttype] = list()\n",
    "    length = len(zs[ttype])\n",
    "    print(length)\n",
    "    for x in range (0, length):\n",
    "        data0 = zs[ttype][x].vh.values.reshape(-1)\n",
    "        data00 = data0[np.isfinite(data0)]\n",
    "        print('Appending to list')\n",
    "    data_final = np.concatenate((data_final, data00))\n",
    "    zs2[ttype].append(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets plot a histogram of the data that has come back from the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minval = 0.001\n",
    "maxval = 0.5\n",
    "bin_values = np.arange(start = minval, stop = maxval, step = 0.001)\n",
    "\n",
    "legend_list = []\n",
    "for ttype in training_types:\n",
    "    plt.hist(zs2[ttype], bins = bin_values, alpha = 0.5)\n",
    "    legend_list.append(ttype)\n",
    "    \n",
    "plt.legend(legend_list)\n",
    "plt.xlabel('Radar vv 250m')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "legend_list = []\n",
    "for ttype in training_types:\n",
    "    data_list.append(zs2[ttype])\n",
    "    legend_list.append(ttype)\n",
    "\n",
    "plt.boxplot(data_list, vert = False)\n",
    "plt.yticks([1, 2, 3, 4, 5, 6], legend_list)\n",
    "plt.xlim(0, 0.5)\n",
    "plt.ylabel('Landcover category')\n",
    "plt.xlabel('Radar vv 250m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
